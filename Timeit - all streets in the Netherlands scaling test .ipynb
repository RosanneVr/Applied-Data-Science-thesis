{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd00ecf47b11f98bfd776c50f2e3d65e1dca601535a3e7ab66bd5a8ac1e4a31b223",
   "display_name": "Python 3.7.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "0ecf47b11f98bfd776c50f2e3d65e1dca601535a3e7ab66bd5a8ac1e4a31b223"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import json\n",
    "import timeit\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import csv\n",
    "import sparql\n",
    "import numpy as np\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://data.netwerkdigitaalerfgoed.nl/_api/datasets/hetutrechtsarchief/Dataset/services/Dataset/sparql\"\n",
    "\n",
    "headers = CaseInsensitiveDict()\n",
    "headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\"\n",
    "\n",
    "resp_list = []  \n",
    "offset = -10000\n",
    "\n",
    "for i in range (128):\n",
    "  for x in range(9):\n",
    "    offset = offset + 10000\n",
    "    queryformat =     \"query=PREFIX%20dc%3A%20%3Chttp%3A%2F%2Fpurl.org%2Fdc%2Felements%2F1.1%2F%3E%0APREFIX%20dct%3A%20%3Chttp%3A%2F%2Fpurl.org%2Fdc%2Fterms%2F%3E%0APREFIX%20sw%3A%20%3Chttp%3A%2F%2Fsemanticweb.cs.vu.nl%2F2009%2F11%2Fsem%2F%3E%0APREFIX%20wd%3A%20%3Chttp%3A%2F%2Fwww.wikidata.org%2Fentity%2F%3E%0ASELECT%20%20%2A%0A%7B%0A%20%20%7B%0A%20%20%20%20select%20%3Fstreet%20%3Fyear%20%28count%20%28%3Fyear%29%20as%20%3Fcount%29%20where%0A%20%20%20%20%7B%0A%20%20%20%20%09%3Fsub%20dct%3Adate%20%3Fyear.%0A%20%20%20%20%20%20%09%3Fsub%20dct%3Aspatial%20%3Fstreet.%0A%20%20%09%09FILTER%20regex%28%3Fstreet%2C%20%22wiki%22%29%0A%09%7Dorder%20by%20%3Fyear%0A%20%20%7D%0A%7D%0A%0ALIMIT%2010000%20offset%20{}%0A%0A\".format(offset)\n",
    "    query = str(queryformat)\n",
    "    resp = requests.post(url, headers=headers, data=query)\n",
    "    respj = resp.json()\n",
    "    resp_list.extend(respj)\n",
    "    offset = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = list(set([item['street'] for item in resp_list]))\n",
    "titles = [x.replace('http://www.wikidata.org/entity/', '') for x in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wiki4():\n",
    "    loc_qry1 = \"\"\"\n",
    "        PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "        PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "        SELECT ?street ?streetLabel ?coords ?bagID ?city ?cityLabel ?citycoords ?s_class WHERE {\n",
    "        VALUES ?street {wd:\"\"\" + \" wd:\".join([i for i in titlelist1]) + \"\"\"}\n",
    "        OPTIONAL {?street wdt:P625 ?coords.}\n",
    "        OPTIONAL {?street wdt:P5207 ?bagID . }\n",
    "        OPTIONAL {?street wdt:P131 ?city . }\n",
    "        ?street wdt:P31 ?s_class .\n",
    "        ?city wdt:P625 ?citycoords.\n",
    "        FILTER(?s_class = wd:Q79007 || ?s_class =  wd:Q2039348 || ?s_class = wd:Q3957)\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],nl\". }\n",
    "        BIND(REPLACE(str(?street), \"http://www.wikidata.org/entity/\", \"\") AS ?location)\n",
    "        }\"\"\"\n",
    "    result = sparql.query(url, loc_qry1)\n",
    "    res = result.fetchall()\n",
    "    res1 = [sparql.unpack_row(row) for row in res]\n",
    "    loc_qry2 = \"\"\"\n",
    "        PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "        PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "        SELECT ?street ?streetLabel ?coords ?bagID ?city ?cityLabel ?citycoords ?s_class WHERE {\n",
    "        VALUES ?street {wd:\"\"\" + \" wd:\".join([i for i in titlelist2]) + \"\"\"}\n",
    "        OPTIONAL {?street wdt:P625 ?coords.}\n",
    "        OPTIONAL {?street wdt:P5207 ?bagID . }\n",
    "        OPTIONAL {?street wdt:P131 ?city . }\n",
    "        ?street wdt:P31 ?s_class .\n",
    "        ?city wdt:P625 ?citycoords.\n",
    "        FILTER(?s_class = wd:Q79007 || ?s_class =  wd:Q2039348 || ?s_class = wd:Q3957)\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],nl\". }\n",
    "        BIND(REPLACE(str(?street), \"http://www.wikidata.org/entity/\", \"\") AS ?location)\n",
    "        }\"\"\"\n",
    "    result = sparql.query(url, loc_qry2)\n",
    "    res = result.fetchall()\n",
    "    res2 = [sparql.unpack_row(row) for row in res]\n",
    "    loc_qry3 = \"\"\"\n",
    "        PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "        PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "        SELECT ?street ?streetLabel ?coords ?bagID ?city ?cityLabel ?citycoords ?s_class WHERE {\n",
    "        VALUES ?street {wd:\"\"\" + \" wd:\".join([i for i in titlelist3]) + \"\"\"}\n",
    "        OPTIONAL {?street wdt:P625 ?coords.}\n",
    "        OPTIONAL {?street wdt:P5207 ?bagID . }\n",
    "        OPTIONAL {?street wdt:P131 ?city . }\n",
    "        ?street wdt:P31 ?s_class .\n",
    "        ?city wdt:P625 ?citycoords.\n",
    "        FILTER(?s_class = wd:Q79007 || ?s_class =  wd:Q2039348 || ?s_class = wd:Q3957)\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],nl\". }\n",
    "        BIND(REPLACE(str(?street), \"http://www.wikidata.org/entity/\", \"\") AS ?location)\n",
    "        }\"\"\"\n",
    "    result = sparql.query(url, loc_qry3)\n",
    "    res = result.fetchall()\n",
    "    res3 = [sparql.unpack_row(row) for row in res]\n",
    "    loc_qry4 = \"\"\"\n",
    "        PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "        PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "        SELECT ?street ?streetLabel ?coords ?bagID ?city ?cityLabel ?citycoords ?s_class WHERE {\n",
    "        VALUES ?street {wd:\"\"\" + \" wd:\".join([i for i in titlelist4]) + \"\"\"}\n",
    "        OPTIONAL {?street wdt:P625 ?coords.}\n",
    "        OPTIONAL {?street wdt:P5207 ?bagID . }\n",
    "        OPTIONAL {?street wdt:P131 ?city . }\n",
    "        ?street wdt:P31 ?s_class .\n",
    "        ?city wdt:P625 ?citycoords.\n",
    "        FILTER(?s_class = wd:Q79007 || ?s_class =  wd:Q2039348 || ?s_class = wd:Q3957)\n",
    "        SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],nl\". }\n",
    "        BIND(REPLACE(str(?street), \"http://www.wikidata.org/entity/\", \"\") AS ?location)\n",
    "        }\"\"\"\n",
    "    result = sparql.query(url, loc_qry4)\n",
    "    res = result.fetchall()\n",
    "    res4 = [sparql.unpack_row(row) for row in res]\n",
    "    res= res1 + res2 +res3 +res4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "titlelist = []\n",
    "\n",
    "limit = 240000\n",
    "\n",
    "queryformat = \"\"\"select ?street {{\n",
    "        ?street wdt:P31 wd:Q79007 .\n",
    "        ?street wdt:P17 wd:Q55 .\n",
    "        SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],nl\". }}\n",
    "        }}LIMIT {}\"\"\".format(limit)\n",
    "query = str(queryformat)\n",
    "result = sparql.query(url, query)\n",
    "res = result.fetchall()\n",
    "result = [sparql.unpack_row(row) for row in res]\n",
    "test = []\n",
    "for i in result:\n",
    "    string = str(i)\n",
    "    title = string.replace('http://www.wikidata.org/entity/', '')\n",
    "    title = title.replace('[', '')\n",
    "    title = title.replace(']', '')\n",
    "    title = title.replace('\\'', '')\n",
    "    titlelist.append(title)\n",
    "\n",
    "length = len(titlelist)\n",
    "half_index = length//2\n",
    "quarter_index = length//4\n",
    "half1 = titlelist[:half_index]\n",
    "half2 = titlelist[half_index:]\n",
    "titlelist1 = half1[:quarter_index]\n",
    "titlelist2 = half1[quarter_index:]\n",
    "titlelist3 = half2[:quarter_index]\n",
    "titlelist4 = half2[quarter_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "double = {\"street\":[],\"year\":[],\"count\":[]}\n",
    "count_df = pd.DataFrame (resp_list,columns=['street', 'year','count'])\n",
    "\n",
    "for index, row in count_df.iterrows():\n",
    "    if len(row.year) > 4:\n",
    "        double[\"street\"].append(row.street)\n",
    "        double[\"year\"].append(row.year)\n",
    "        double[\"count\"].append(row.count)\n",
    "        row.year = np.nan\n",
    "\n",
    "count_df.dropna(subset=['year'])\n",
    "\n",
    "doubledf = DataFrame.from_dict(double)\n",
    "doubledf[['start_date','end_date']] = doubledf.year.str.split(\"/\",expand=True,)\n",
    "doubledf['start_date'] = doubledf['start_date'].astype(int)\n",
    "doubledf['end_date'] = doubledf['end_date'].astype(int)\n",
    "\n",
    "extraY = {\"street\":[],\"year\":[],\"count\":[]}\n",
    "year_range = []\n",
    "\n",
    "for row in doubledf.itertuples():\n",
    "    year_range = range(row.start_date , row.end_date+1)\n",
    "    for i in year_range: \n",
    "        extraY[\"street\"].append(row.street)\n",
    "        extraY[\"year\"].append(i)\n",
    "        extraY[\"count\"].append('1')\n",
    "\n",
    "extraYdf = DataFrame.from_dict(extraY)\n",
    "\n",
    "result = count_df.append(extraYdf, sort=False)\n",
    "result['count'] = result['count'].astype(int)\n",
    "count_result = result.groupby(['street','year']).agg(count = pd.NamedAgg(column='count', aggfunc='sum')).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_qry1 = \"\"\"\n",
    "    PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "    PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "    SELECT ?street ?streetLabel ?coords ?bagID ?city ?cityLabel ?citycoords ?s_class WHERE {\n",
    "    VALUES ?street {wd:\"\"\" + \" wd:\".join([i for i in titlelist1]) + \"\"\"}\n",
    "    OPTIONAL {?street wdt:P625 ?coords.}\n",
    "    OPTIONAL {?street wdt:P5207 ?bagID . }\n",
    "    OPTIONAL {?street wdt:P131 ?city . }\n",
    "    ?street wdt:P31 ?s_class .\n",
    "    ?city wdt:P625 ?citycoords.\n",
    "    FILTER(?s_class = wd:Q79007 || ?s_class =  wd:Q2039348 || ?s_class = wd:Q3957)\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],nl\". }\n",
    "    BIND(REPLACE(str(?street), \"http://www.wikidata.org/entity/\", \"\") AS ?location)\n",
    "    }\"\"\"\n",
    "result = sparql.query(url, loc_qry1)\n",
    "res = result.fetchall()\n",
    "res1 = [sparql.unpack_row(row) for row in res]\n",
    "loc_qry2 = \"\"\"\n",
    "    PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "    PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "    SELECT ?street ?streetLabel ?coords ?bagID ?city ?cityLabel ?citycoords ?s_class WHERE {\n",
    "    VALUES ?street {wd:\"\"\" + \" wd:\".join([i for i in titlelist2]) + \"\"\"}\n",
    "    OPTIONAL {?street wdt:P625 ?coords.}\n",
    "    OPTIONAL {?street wdt:P5207 ?bagID . }\n",
    "    OPTIONAL {?street wdt:P131 ?city . }\n",
    "    ?street wdt:P31 ?s_class .\n",
    "    ?city wdt:P625 ?citycoords.\n",
    "    FILTER(?s_class = wd:Q79007 || ?s_class =  wd:Q2039348 || ?s_class = wd:Q3957)\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],nl\". }\n",
    "    BIND(REPLACE(str(?street), \"http://www.wikidata.org/entity/\", \"\") AS ?location)\n",
    "    }\"\"\"\n",
    "result = sparql.query(url, loc_qry2)\n",
    "res = result.fetchall()\n",
    "res2 = [sparql.unpack_row(row) for row in res]\n",
    "loc_qry3 = \"\"\"\n",
    "    PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "    PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "    SELECT ?street ?streetLabel ?coords ?bagID ?city ?cityLabel ?citycoords ?s_class WHERE {\n",
    "    VALUES ?street {wd:\"\"\" + \" wd:\".join([i for i in titlelist3]) + \"\"\"}\n",
    "    OPTIONAL {?street wdt:P625 ?coords.}\n",
    "    OPTIONAL {?street wdt:P5207 ?bagID . }\n",
    "    OPTIONAL {?street wdt:P131 ?city . }\n",
    "    ?street wdt:P31 ?s_class .\n",
    "    ?city wdt:P625 ?citycoords.\n",
    "    FILTER(?s_class = wd:Q79007 || ?s_class =  wd:Q2039348 || ?s_class = wd:Q3957)\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],nl\". }\n",
    "    BIND(REPLACE(str(?street), \"http://www.wikidata.org/entity/\", \"\") AS ?location)\n",
    "    }\"\"\"\n",
    "result = sparql.query(url, loc_qry3)\n",
    "res = result.fetchall()\n",
    "res3 = [sparql.unpack_row(row) for row in res]\n",
    "loc_qry4 = \"\"\"\n",
    "    PREFIX wikibase: <http://wikiba.se/ontology#>\n",
    "    PREFIX bd: <http://www.bigdata.com/rdf#>\n",
    "\n",
    "    SELECT ?street ?streetLabel ?coords ?bagID ?city ?cityLabel ?citycoords ?s_class WHERE {\n",
    "    VALUES ?street {wd:\"\"\" + \" wd:\".join([i for i in titlelist4]) + \"\"\"}\n",
    "    OPTIONAL {?street wdt:P625 ?coords.}\n",
    "    OPTIONAL {?street wdt:P5207 ?bagID . }\n",
    "    OPTIONAL {?street wdt:P131 ?city . }\n",
    "    ?street wdt:P31 ?s_class .\n",
    "    ?city wdt:P625 ?citycoords.\n",
    "    FILTER(?s_class = wd:Q79007 || ?s_class =  wd:Q2039348 || ?s_class = wd:Q3957)\n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE],nl\". }\n",
    "    BIND(REPLACE(str(?street), \"http://www.wikidata.org/entity/\", \"\") AS ?location)\n",
    "    }\"\"\"\n",
    "result = sparql.query(url, loc_qry4)\n",
    "res = result.fetchall()\n",
    "res4 = [sparql.unpack_row(row) for row in res]\n",
    "res= res1 + res2 +res3 +res4\n",
    "bb_locs = pd.DataFrame.from_records(res, columns=result.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "municipality_df = pd.merge(count_result,bb_locs[['street','city', 's_class']],on='street', how='left')\n",
    "provinces = ['http://www.wikidata.org/entity/Q2039348', 'http://www.wikidata.org/entity/Q3957']\n",
    "province_df = municipality_df[municipality_df.s_class.isin(provinces)]\n",
    "city_df = municipality_df[~municipality_df.s_class.isin(provinces)]\n",
    "province_df= province_df[['street', 'year', 'count']]\n",
    "city_df = city_df[['city','year', 'count']]\n",
    "province_df = province_df.rename(columns = {'street':'city'})\n",
    "city_df = city_df.dropna()\n",
    "all_municipalities = city_df.append(province_df, sort=False)\n",
    "count_municipalities = all_municipalities.groupby(['city','year']).agg(count = pd.NamedAgg(column='count', aggfunc='sum')).reset_index()"
   ]
  },
  {
   "source": [
    "## All definitions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hualab():\n",
    "    url = \"https://data.netwerkdigitaalerfgoed.nl/_api/datasets/hetutrechtsarchief/Dataset/services/Dataset/sparql\"\n",
    "\n",
    "    headers = CaseInsensitiveDict()\n",
    "    headers[\"Content-Type\"] = \"application/x-www-form-urlencoded\"\n",
    "\n",
    "    resp_list = []  \n",
    "    offset = -10000\n",
    "\n",
    "    for i in range (128):\n",
    "        for x in range(9):\n",
    "            offset = offset + 10000\n",
    "            queryformat =     \"query=PREFIX%20dc%3A%20%3Chttp%3A%2F%2Fpurl.org%2Fdc%2Felements%2F1.1%2F%3E%0APREFIX%20dct%3A%20%3Chttp%3A%2F%2Fpurl.org%2Fdc%2Fterms%2F%3E%0APREFIX%20sw%3A%20%3Chttp%3A%2F%2Fsemanticweb.cs.vu.nl%2F2009%2F11%2Fsem%2F%3E%0APREFIX%20wd%3A%20%3Chttp%3A%2F%2Fwww.wikidata.org%2Fentity%2F%3E%0ASELECT%20%20%2A%0A%7B%0A%20%20%7B%0A%20%20%20%20select%20%3Fstreet%20%3Fyear%20%28count%20%28%3Fyear%29%20as%20%3Fcount%29%20where%0A%20%20%20%20%7B%0A%20%20%20%20%09%3Fsub%20dct%3Adate%20%3Fyear.%0A%20%20%20%20%20%20%09%3Fsub%20dct%3Aspatial%20%3Fstreet.%0A%20%20%09%09FILTER%20regex%28%3Fstreet%2C%20%22wiki%22%29%0A%09%7Dorder%20by%20%3Fyear%0A%20%20%7D%0A%7D%0A%0ALIMIT%2010000%20offset%20{}%0A%0A\".format(offset)\n",
    "            query = str(queryformat)\n",
    "            resp = requests.post(url, headers=headers, data=query)\n",
    "            respj = resp.json()\n",
    "            resp_list.extend(respj)\n",
    "            offset = -10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titles():\n",
    "    titles = list(set([item['street'] for item in resp_list]))\n",
    "    titles = [x.replace('http://www.wikidata.org/entity/', '') for x in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversion():\n",
    "\n",
    "    double = {\"street\":[],\"year\":[],\"count\":[]}\n",
    "    count_df = pd.DataFrame (resp_list,columns=['street', 'year','count'])\n",
    "\n",
    "    for index, row in count_df.iterrows():\n",
    "        if len(row.year) > 4:\n",
    "            double[\"street\"].append(row.street)\n",
    "            double[\"year\"].append(row.year)\n",
    "            double[\"count\"].append(row.count)\n",
    "            row.year = np.nan\n",
    "\n",
    "    count_df.dropna(subset=['year'])\n",
    "\n",
    "    doubledf = DataFrame.from_dict(double)\n",
    "    doubledf[['start_date','end_date']] = doubledf.year.str.split(\"/\",expand=True,)\n",
    "    doubledf['start_date'] = doubledf['start_date'].astype(int)\n",
    "    doubledf['end_date'] = doubledf['end_date'].astype(int)\n",
    "\n",
    "    extraY = {\"street\":[],\"year\":[],\"count\":[]}\n",
    "    year_range = []\n",
    "\n",
    "    for row in doubledf.itertuples():\n",
    "        year_range = range(row.start_date , row.end_date+1)\n",
    "        for i in year_range: \n",
    "            extraY[\"street\"].append(row.street)\n",
    "            extraY[\"year\"].append(i)\n",
    "            extraY[\"count\"].append('1')\n",
    "\n",
    "    extraYdf = DataFrame.from_dict(extraY)\n",
    "\n",
    "    result = count_df.append(extraYdf, sort=False)\n",
    "    result['count'] = result['count'].astype(int)\n",
    "    count_result = result.groupby(['street','year']).agg(count = pd.NamedAgg(column='count', aggfunc='sum')).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citycount():\n",
    "    municipality_df = pd.merge(count_result,bb_locs[['street','city', 's_class']],on='street', how='left')\n",
    "    provinces = ['http://www.wikidata.org/entity/Q2039348', 'http://www.wikidata.org/entity/Q3957']\n",
    "    province_df = municipality_df[municipality_df.s_class.isin(provinces)]\n",
    "    city_df = municipality_df[~municipality_df.s_class.isin(provinces)]\n",
    "    province_df= province_df[['street', 'year', 'count']]\n",
    "    city_df = city_df[['city','year', 'count']]\n",
    "    province_df = province_df.rename(columns = {'street':'city'})\n",
    "    city_df = city_df.dropna()\n",
    "    all_municipalities = city_df.append(province_df, sort=False)\n",
    "    count_municipalities = all_municipalities.groupby(['city','year']).agg(count = pd.NamedAgg(column='count', aggfunc='sum')).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export():\n",
    "    count_municipalities.to_csv('CityCountUtrechtsArchief.csv')\n",
    "    count_result.to_csv(\"LocationCountUtrechtsArchief.csv\")\n",
    "    bb_locs.to_csv(\"Wikidatalocations.csv\")"
   ]
  },
  {
   "source": [
    "## Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1\n",
    "\n",
    "time_hualab = []\n",
    "time_titles = []\n",
    "time_wikidata = []\n",
    "time_conversion = []\n",
    "time_citycount = []\n",
    "time_export = []\n",
    "\n",
    "for i in range(20):\n",
    "    time_hualab.append(timeit.Timer(hualab).timeit(number=iterations))\n",
    "    time_titles.append(timeit.Timer(titles).timeit(number=iterations))\n",
    "    time_wikidata.append(timeit.Timer(Wiki4).timeit(number=iterations))\n",
    "    time_conversion.append(timeit.Timer(conversion).timeit(number=iterations))\n",
    "    time_citycount.append(timeit.Timer(citycount).timeit(number=iterations))\n",
    "    time_export.append(timeit.Timer(export).timeit(number=iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Retrieve data from Utrecht Archives:', time_hualab,'\\nMean:',statistics.mean(time_hualab), '\\nStandard deviation:',statistics.stdev(time_hualab),'\\nMax:',max(time_hualab), '\\n\\nCreate title list:', time_titles,'\\nMean:',statistics.mean(time_titles),'\\nStandard deviation:',statistics.stdev(time_titles),'\\nMax:',max(time_titles),  '\\n\\nRetrieve data from Wikidata:', time_wikidata,'\\nMean:', statistics.mean(time_wikidata),'\\nStandard deviation:',statistics.stdev(time_wikidata),'\\nMax:',max(time_wikidata),  '\\n\\nConvert double years notation to periods:', time_conversion,'\\n Mean:', statistics.mean(time_conversion),'\\nStandard deviation:',statistics.stdev(time_conversion),'\\nMax:',max(time_conversion),  '\\n\\nCreate city count:', time_citycount, '\\nMean:', statistics.mean(time_citycount),'\\nStandard deviation:',statistics.stdev(time_citycount), '\\nMax:',max(time_citycount),'\\n\\nExport files to CSV:', time_export, '\\nMean:', statistics.mean(time_export),'\\nStandard deviation:',statistics.stdev(time_export),'\\nMax:',max(time_export))"
   ]
  }
 ]
}